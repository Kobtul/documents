\chapter{Experiments}\label{ch:experiments}

In this chapter we describe an experimentation with the method derived in chapter
\ref{ch:method}. Software \code{PyNfSA} has been created using 
Python Programming Language~\cite{rossum2011python} for
experimentation purposes. It is intended
to be a set of tools to provide convenient environment 
for performing signal analysis on the network traffic data. 

In following sections we introduce data acciusition process (section~\ref{sec:data})
as well as the process of signal analysis using \code{PyNfSA} (section~\ref{sec:sw}).
Finally we depict the results of the experimentation on real data (section~\ref{sec:results}).

\section{Data gathering} \label{sec:data}

For training and evaluation purposes several datasets has been used. We took in account
\emph{packet trace} data as well as \emph{traffic flow statistics}. 

For the capure and storage of the packet \emph{trace data} we use \code{PCAP} format.
This format is used by open-source library \code{libpcap}~\cite{jacobson2009libpcap}, 
that is available for wide range of platforms (Windows, Linux, BSD, etc.) and 
has bindings to several programming laguages (C, perl, python, etc.).

For \emph{traffic flow statistics}  Cisco Systems developed an 
\code{NetFlow} format and communication protocol.
It has now wide range of equivalent implementations, e.g. \code{JFlow} 
(developped by Juniper Networks), and there also exists software to generate 
\code{NetFlow} from \code{PCAP} files.

\emph{Cisco Systems NetFlow v. 9}  protocol is published in RFC3954~\cite{claise2007rfc}.
The publication contains information about NetFlow data gathering and exchange process, as well
as the specification of data attributes contained within dataset.
However the specification is extensive, and for our experiments we use only negligable 
subset of the attributes as has been defined in section~\ref{sec:collect}.

For internal representation of the data we use matrices and array databases.
Our experimental software uses \code{HDF5} suite~\cite{folk2011overview}
for storage of the data. This suite consits of several open-source libraries, tools and 
bindings to various languages.
%this is irrelevantIt is used also by Matlab or Mathematica.
Our experimantal software is capable of conversion from \code{PCAP} or \code{NetFlow} 
formats to matrix representation in \code{HDF5} files.

\subsection{Simulation}
Virtual testbed has been used to simulate traffic in a low scale.
A \code{tcpdump}~\cite{jacobson2009tcpdump} software has been used to 
capture packet data in experimental testbed.
Testbed consist of three or more virtual computers. Communication between
hosts has been routed trought one of them where \code{tcpdump} has been used
to capture the communication in the tesbed. The \code{tcpdump} captures every
packet passing trough the interfaces and stores the data in \code{PCAP} ~\cite{jacobson2009libpcap} file.
Each captured packet has been truncated to 200 bytes before storing.

Various scenarios has been simulated in the testbed  in small scale: 
\emph{eperimantal mallware-like
software}, \emph{file tranfers}, \emph{text terminal sessions}, 
\emph{tunneled connections}, \emph{virtual private network (VPN)  connections}.%
%\footnote{
%    In this case the malware has been sending information to remote server
%    using HTTP or HTTPS protocols. The amount of data were
%    inverse proportional to delay between transmissions and the delay 
%    has been chosen at random.
%}%
%. 

The experimental mallware-like software called \code{woodpecker} 
has been created for purposes of present work.
It has been intended to simulate behavior of the malicious agent sending and receiving information to
remote server using HTTP protocol. The delay between consecutive requests 
as well as the size of the payload has been randomized. 
At protocol level an agent has not been intended to violate specification, 
thus standard implementation of HTTP protocol has been used.
At least ten instances of the \code{woodpecker} operated over several days 
in eperimental testbed simulating several attacked hosts.

For tunneled connection an \code{httptunnel} software has been used. This softvare provides
data link between two endpoint encapsulating the data to payloads of the HTTP requests and 
responses. A random data has been used to pass trought the tunnel.

For the VPN connection an \code{OpenVPN} software has been used. It is open-source and
widely used VPN client and server. It has been used to tunnel the user traffic
including world wide web, terminal sessions, or the file transfers. The behavior of the
traffic encapsulated in tunnel has been changing. The were present single connection as well
as multiplexed ones. It has been also used to pass troughout the HTTP proxy server 
to circumvent security restrictions. 
The VPN server uses an HTTPS service TCP port and is intended to be undistinguishable 
from the HTTPS service by the port number. 
However there are signature-based methods used to detect such behavior,
it uses inspection of the packet payload. We are not concerned with this option
as we intend to build an anomaly detection method instead that doues not have 
the packet payload at the input.

Simulated data has been mixed with data obtained from other sources. Simulated 
malicious activities has been subject to test the performance of the method.


%\subsection{Anonymized public data}
%
%There are wide range of datasets available in public domain, however improper 
%selection of dataset can bias the results of the experiments. 
%
%By reseraching in papers related to IDS systems, references to public datasets has been found.
%We took into consideration few of them:
%
%\emph{DARPA Intrusion Detection Data Sets}~\cite{darpa1999ids} are dataset captred in artificial
%environmnet simulated in laboratory. \name{McHugh}  raised critics against this datasets about
%the extent to which the data is appropiate for evaluation of IDS systems.
%He noticed that the architecture has been developed from the attacker's point of view which
%can lead in biased evaluation.
%
%One of the most used datasets are published by \emph{The Cooperative Association for Internet 
%Data Analysis (CAIDA)}. Published datasets, that are in our interest are 
%\emph{CAIDA Anonymized Internet Traces 2011}~\cite{caida2011trace},
%captured at internet backbone links in Chicago, US.
%The data contains timestamped anonymized packet headers in \code{PCAP} format.

\subsection{Anonnymized non-public data}\label{susec:non-pub}

Two evaluation datasets has been used. One has been captured on campus routers
and has been provided by this work`s supervisor. The data format used an \code{NetFlow} specification
to gather the data. It was an capture of flow statistics at the campus network over
two weeks. An 5 minute sampling period has been used. An open-source implementation
of \code{NetFlow} standards is the \code{nfdump}~\cite{haag2006netflow} software.
It has several options of data formatting and most convenient for further processing
is the line text format as the parser should be implemented ass the single regular expression.

Another datased has been captured at different locations and environment using a \code{tcpdump} 
software providing a \code{PCAP} format. The data comprises typical use of the computer network
as the world wide web (\code{WWW}), terminal session (\code{SSH}, \code{telnet}, \code{RDP}), 
file transfers (\code{SCP}, \code{CIFS}) etc. It has been collected over period of one week.

\subsection{Labeling}\label{subsec:labeling}

Labeling of the data is performed in manual manner involving expert knowledge.
For the simulated traffic, labels are easy to assign as they are known during the simulation.
Labeling data gathered in real network over longer period of time
requires huge effort. 

An approach often used, %TODO cite
is to use  state-of-the-art intrusion detection system to perform analysis and obtain
labeling from its output, however labelling obtained  this way
can be biased especially for new unknown malicious behaviors that is subject to research.

The labeling of the data is needed for training in case of supervised or semi-supervised
anomaly detection methods and it is always needed for evaluation of the performance of methods
(even unsupervised).

In case the labels are not coresponding to reality it can bias the model fitting process and 
thus decrease performance of the model. Altought there are model fitting methods that are 
robust to mislabelled sample, it is important to obtain sample with correct labeling
information for model finding and evaluation purposes. The evaluation of the robustness
of such method has to be performed separately.

In present work labeling of captured data has been derived using manual inspection by human expert.
Labeling o the simmulated attacks has been know at the time of the data capture.


\section{Implementation} \label{sec:sw}

For purposes of exeperimentation an \code{PyNfSA}~\cite{boraros2012PyNfSA} 
software has been created. It integrates several libraries in Python Programming language
to provide computations introduced in chapter~\ref{ch:method}. The solution is built
around \code{numpy}~\cite{oliphant2006numpy} library. Complete list of dependecies as well as the
usage and development manual is included in \emph{PyNfSA Documentation} in Appendix~\ref{ch:appa}.
Let us introduce the implementation details of the \code{PyNfSA}  software in terms 
described in chapter~\ref{ch:method}:

Data accuisition requires \code{libpcap} ~\cite{jacobson2009libpcap} library for capturing packet data 
or the \code{nfdump}~\cite{haag2006netflow}
software used for capture netflow statistics. The \code{nfdump} software has several output formats
but the \code{PyNfSA} software relies on ``long'' line format as it can be easily parsed using reqular 
expression. The \code{PyNfSA} reads and converts accuired data into internal matrix representation.
The matrices are then stored using \code{HDF5} technology~\cite{folk2011overview} for later use. 
When parsing the \code{PCAP} or the \code{NetFlow} format, only subset of information is 
selected as specified in subsection~\ref{susec:non-pub}.

Next step is to compute \emph{flow matrix}. Rows in flow matrix represents data instances comprising
timestamp, packet count and size, direction and flow identification. 
The flow identification is based on \emph{flow 5-tuple} 
i.e. tranmission \emph{protocol} speciffication, \emph{address} and \emph{port} of
source and destination endpoint. When modeling behavior of specific source endpoint with 
respect to specific application service, it is usefull to discard source port information in some cases.
The reason is that the application client side often requests an socket from operating system
to the destination server, performs request-response roundtrip and closes the socket.
For succesive request new sockets are openned. Based on implementation of the traffic flow 
protocol in operating system new source port is assigned. In our analysis we consider that succesive 
connections are related together. This is the case especially for HTTP application protocol,
the client side often creates new socket for each small set of requests. As the HTTP application protocol
are subject to our research our flow identification scheme does not include the source port.

After flow matrix is computed it is stored in \code{HDF5} file for later analysis. Such representation of
the data is very compact when compared to netflow or to the pcap format, as it is constructed 
based on pre-defined set of the attributes. In addition an \code{HDF5} technolgy allows indexing of the 
array data and thus filtering in the data is often faster than in raw formats.
Drawback of this representation is that addition of new attribute means processing of 
raw data and re-creation of whole matrix.

From the flow matrix an \emph{feature matrix} is constructed. Features are created by computation of \emph{power
spectral density} in defined time window and using specified sample rate. 
%This proces requires extensive filtering of the flow matrix so the efective implementation is needed.
 Each row in feature matrix represents
single data instance comprising the frequency components. A flow identification is assigned to each 
sample and it stored as a vector along with a feature matrix.
While in flow matrix a time context is still present, it is not present in in feature matrix,
the only relation between samples is the flow identification.

The labeling is implemented using filtering based on \emph{flow 5-tuple}.
The labels are assigned based on filters selecting the subset of flows identifications.
A labeling vector is stored along with the flow identification vector and the feature matrix.
Filters are created manualy, \code{JSON} media type~\cite{crockford2006application} is used. The format of the 
filter is specified in \emph{PyNfSA Documentation}.

The model fitting and evaluation uses $k$-fold stratified cross validation. 
During model fitting phase an training sample undergoes transformations in order to reduce dimensionality
and fitting is performed on transformed sample. A model is then used to obtain raw score.
For unimodal Gausian distribution an Mahalanobis distance is used, for Gaussian Mixture Model and
log-probability is used as raw score. The fitting of the unimodal Gausian distribution  for 
Mahalanobis distance computations and robust covariance matrix estimation can be used. 
The  computation then involves Minimum Covariance Determinant~\cite{rousseeuw1984least}.

For the machine learning procedures and \code{SciKit} library~\cite{pedregosa2011scikit} has been used.
It is set of classes in Python Programming Language that implements lot of algorithms for
machine learning and statistical analysis. Few extensions to the computational models has been 
performed during the \code{PyNfSA} development.

The \code{PyNfSA} allows entering to interactive mode. Interactive mode is used to invoke analysis
``on-the-fly'' using an ad-hoc commands on the commad line. For interactive mode \code{iPython}~\cite{iPython} 
interactive shell  is  used. It allow to use syntax similar to \code{Matlab}, 
\code{Octave} or even \code{R} by invoking special commands. In addition \code{HDF5} data file
can be used directly by \code{Matlab} or \code{Octave} as it is natively supported format,


%\cite{rossum2011python,oliphant2006numpy,pedregosa2011scikit}

%In experiments we focused on the application protocols that are most 
%spread in computer networks and that are thus most widely miss-used. 
%In particular we will analyze protocol HTTP and its encrypted version -- HTTPS. 
%Most of the network gateways allow only usage of network protocols HTTP or HTTPS. 
%As protocol HTTPS uses encryption, the gateway is unable to detect miss-use by 
%analyzing content and thus it is unable to enforce policy restrictions.
