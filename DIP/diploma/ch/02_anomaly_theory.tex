
\chapter{Anomaly Detection}

%% this is intended to be brief, so keep it simple and general
In general an anomaly detection is the problem of finding patterns in data that do not 
conform to expected behavior.
A term \emph{anomaly} or \emph{outlier} refers to these non-conforming patterns. 
Usually knowledge about non-conforming patterns is inportant due to fact that they may refer
to singnificant information, in many cases also critical and actionable, 
e.g. a tumor presence may be indicated by anomalous magnetic resonance imaging (MRI) scan, 
network intrusion may cause anomalous signature of the packets to be observed.

%% history
The anomaly detection has been studied as early as the $19^{th}$ century by 
statisticians as a statistical method.
Due now, several techniques have been developed, using domain-independent approach 
or developed specificaly for particular domain. %TODO moar faking history

Apparently simple approach of anomaly detection is to define a region representing 
normal behavior and declare any patterns which does not conform to this region as anomaly. 
This approach is obfuscated by several factors:
\begin{itemize}
	\item Definition of normal behavior must contain every possible normal behavior 
	and it is difficultly achievable.
	\item The boundary between anomalies and normal behavior is not accurate and 
	can introduce wrong interpretation	of particular patterns laying near the boundary.
	\item Adaptation of malicious agents to make their outcomes appear like normal in 
	given feature space.
	%TODO \footnote{Adaptation of the agents may }
	\item Normal behavior is evolving in time and thus an normal model defined in one time span 
	can be inaccurate or invalid in future.
	\item An ammount of labeled data needed for derivation of the normal model is insuficient.
	\item Presence of the noise that can  be similar as anomalies, and thus it 
	can be difficult to suppress. 
	\item Different application domains have different notion of an anomaly, 
	thus development of domain-indepedent method is complicated.
\end{itemize}

In general the anomaly detection problem is difficult to solve. 
Most techniques solve a specific formulations of the problem, induced by a 
factors specific for a particular domain. The anomaly detection techniques itself
were developed by adoption of the concepts from diverse disciplines such as \emph{statistics}, 
\emph{machine learning}, \emph{data mining}, \emph{information theory}, \emph{spectral theory}.

\section{Input Data}

Input is generally a collection of data instances, referred as \emph{pattern}, 
\emph{sample} or \emph{observation}.
Each data instance is represented by non-empty set of attributes, also refered as 
\emph{variable} or \emph{feature}.
Attributes can be instances of different data types e.g. \emph{continous}, 
\emph{cathegorical}, or \emph{binary}.
Furthermore in case of each data instance consist of single attribute it is reffered to as
\emph{univariate} otherwise it is \emph{multivariate}. 
For multivariate instances the data types of the attributes might be mixed as well as 
the domain of definition might be different.

\subsection{Relationship Among Data Instances}

Based on presence of the relationship in data, the input data can be further categorized as
\emph{point data}, \emph{sequence data}, \emph{spatial data}, and \emph{graph data}. 
In point data no relationship is assumed among the instances.
In sequence data, presence of the \emph{total order relation}%
\footnote{%
 	In set theory a \emph{total order} is a binary relation on some set $X$. 
	The relation of total order is defined by axioms of \emph{antisymetry}, \emph{transitivity} 
	and \emph{totatlity}. Total order is usually denoted as $\le$.
} %
among data instances is assumed. The sequence data can be time-series, protein sequences, etc.
In \emph{spatial data} presence of \emph{metric}%
\footnote{
	Metric, or distance function, is a non-negative function which defines distance or 
	similarity between elements of the set. Metric is required to satisfy axioms of
	\emph{coincidence}, \emph{symmetry} and \emph{triangle inequality}.
	A metric space is mathematical structure $(X,d)$, where $X$ is a set and function  
	$d:X \times X \rightarrow \mathbb{R}$ is a metric. 
} %
is required. 
The metric determines an neighbourhood of each data instance. The examples of metrics are
\emph{Minkowski metric}% 
\footnote{
	Minkowski metric, defined as $ d(x,y) = (\sum_{i=1}^n(x_i-y_i)^k )^\frac{1}{k}$, 
	is a distance between $n$-vectors $x$ and $y$.
	By choosing value of parameter $k=1$ we get a Mahattan or  a Hamming distance, 
	for $k=2$ we get an Euclid distance, or for $k=\infty $ we get a Chebyshev distance.
} %
(e.g. \emph{Euclidean} distance or \emph{Manhattan} distance), \emph{Levenshtein distance}
(editation distance between strings of characters). Typical example of spatial data is the
coordinate in geographic coordinate system or, asuming our definition, also textual data 
(notice that Levenshtein distance is \emph{metric} among the strings of characters).
The \emph{graph data} instances are represented by graph structure%
\footnote{
	In most common sense, a \emph{graph} $G$ is mathematical structure $G=\left(V,E\right)$
	comprising a set of vertices $V$ with set of edges $E$.
	Edges can be two-element subsets of  $V$ (undirected graph) or ordered pairs 
	of elements of $V$ (directed graph).
	In addition if \emph{weight function} -- $w:E\rightarrow \mathbb{R} $ is defined, 
	assigning a number (e.g. weight, price, etc.) to each edge, we call structure
	$G=\left( V,E,w \right)$ a \emph{weighted graph}.
}%
. As an example of the graph data is map of social social interactions.

In case context are mixed we refer to spatio-temporal (e.g. climate data) or 
graph-temporal data (computer network packet flows).

\subsection{Data Labels}

Labels associated with particular data instances denote if instance is \emph{anomalus}
or \emph{normal}. Labeling is often done by human expert hence it is very expensive and requires
huge effort. Obtaining labels for all possible normal behavior is often less difficult
than obtaining labels for anomalous behavior. Moreover, anomalous behavior is dynamic so
new types of the anomalies might originate. Newly formed anomalies might be then missing 
from models and hence might elude undetected in detection process. 

\section{Anomalies}

Based on presence of the relationship between data instances and problem formulation, anomalies
can be divided into \emph{point anomalies}, \emph{contextual anomalies} and \emph{collective anomalies}.

\textbf{Point anomalies.} In the simplest case, if an individual data instance is considered as
anomalous with respect to the rest of data. No information about relationship between data instances
is assumed. This type of anomaly is target of most of the research studies.
%TODO include examples and pictures

\textbf{Contextual anomalies.} In many cases, an context is present in data set. 
Context is induced by the structure of the data. In case a data instance is anomalous only whithin
a given context, it is called \emph{contextual anomaly}. 
The notion of the context has to be specified within problem formulation. 
By introducing the context in data, the features are divided to \emph{contextual features}
and \emph{behavioral features}.

The \emph{contexutal features} are used to determine the context for 
particular data instance. As an examples of the contextual features are: a timestamp denoting 
temporal context in sequential data, a geographic coordinate denoting spatial context.

The \emph{behavioral features} define non-contextual characteristics of an instance. For example, 
the number of arrived packets during network communication whithin a specific time span is considered 
as an behavioral attribute. Identical data instances (in terms of behavioral attributes) 
may be considered as anomalous or non-anomalous in a different contexts.

%TODO some graphics

\textbf{Collective anomalies.} If a collection of related data instances is anomalous with respect
entire dataset it is called \emph{collective anomaly}. The collective anomaly is defined only 
in data set where an relationship among instances are related, e.g. in sequence data, graph data or
spatial data.
\paragraph*{}
It is important to note that \emph{point anomalies} can occur in any data set, while 
\emph{contextual anomalies} depend on notion of the context and its definition in problem formulation, 
and \emph{collective anomalies} are relevant for data where relationship among instances is defined
(e.g. distance metric). So by taking in account the context information a point or collective anomaly
detection problem can be converted into contextual anomaly detection problem.
%TODO example needed

\section{Techniques}

Availability of the data labeling singnificantly affects usability of particular anomaly technique. 
Based on extent, in which labels are present in data, following modes of operation of anomaly detection 
techniques are available: \emph{supervised}, \emph{semi-supervised} and \emph{unsupervised}.

\textbf{Supervised anomaly detection} assumes availability of labeled data, for normal and also for 
anomaly class. This approach has two major issues. First is that it requires model for both -- the normal
and the anomalous instances. As the anomalous instances are less frequent it takes huge effort to 
obtain and to label data instances for all possible anomalous behaviors. Secondly it is an problem of
imbalanced class distribution, that has been addressed by several studies, e.g. \cite{chawla2004editorial},
\cite{phua2004minority} or \cite{joshi2002predicting}.
%TODO explain the problem of imbalanced class distribution

\textbf{Semi-Supervised anomaly detection} assumes availability of data labels only for one class. 
Generaly, it is not easy nor possible to model anomalies as this might entail previously unseen 
catastrophic event, to be predicted using the given method.
Thus the wast majority of {semi-spervised} techniques model normal behavior.

\textbf{Unsupervised anomaly detection} do not require training data. The unsupervised 
techniques are based on assumption that normal instances are more frequent or have densier distribution
than anomalous.	


Typically, the output produced by anomaly detection technique are \emph{scores} or \emph{labels}.
%
\textbf{Score} is assigned to each data instance depending on the degree to which that instance is 
considered as anomaly.
\textbf{Label} is assigned to each data instance to distinguish normal or anomalous instance.

Based on used method, techniques can be divided to following categories: \emph{classification},
\emph{nearest neighbor}, \emph{clustering}, \emph{statistical}, \emph{information teoretic},
\emph{spectral decomposition}%
\footnote{
	\emph{Spectral decomposition} refer to cannonical decomposition provided under 
	\emph{spectral theorem}, called also \emph{eigendecomposition}.
	However, in present work we are concerned with different decomposition -- a 
	\emph{Fourier transform} which is part of spectral theory as well. 
	The \emph{Fourier transform} is used 
	in field of statistical signal processing to transform time-domain signal into frequency-domain.
	
}%
, etc. Some of the techniques are inherently \emph{unsupervised}, e.g. \emph{nearest neighbor} and 
\emph{clustering} based.
\begin{itemize}
	\item clasification 
	\begin{itemize}
		\item rule 
		\item bayesian network 
		\item support vector machines
		\item neural networks
	\end{itemize}
	\item nearest neighbor
	\begin{itemize}
		\item density  
		\item distance
	\end{itemize}
	\item clustering
	\item statistical
	\begin{itemize}
		\item parametric  
		\item nonparametric
	\end{itemize}
	\item information theoretic
	\item spectral decomposition
\end{itemize}

\section{Evaluation of Outlier Detection}

\section{Application domain}
